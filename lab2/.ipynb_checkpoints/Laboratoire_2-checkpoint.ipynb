{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Wu0knw5vnGAJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from utils.data import prepare_data\n",
    "from utils.gini import eval_gini, gini_xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifiers\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensemble methods (bagging and/or boosting)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DagTeZ_ANDgk"
   },
   "source": [
    "# Laboratoire 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRO_XBj9M56w"
   },
   "source": [
    "Comme vu en classe, la classification est une part importante de la science des données. Elle consiste à apprendre à classer des échantillons et de par la suite prédire les classes de nouveaux échantillons. \n",
    "\n",
    "Durant ce laboratoire, vous allez travailler sur un concours Kaggle dont le but est de prédire la probabilité qu'un conducteur fasse une réclamation d'assurance auto durant l'année suivante. En voyant ce problème, la tâche semble être celle d'une regression et non d'un classification, cependant, avec les données disponibles, c'est effectivement une classification. En effet, les cibles ne sont pas des réels mais bien des entiers naturels puisque l'assureur peut simplement noter si le client a oui ou non fait une réclamation durant l'année d'après. Mais je vous conseille d'aller sur https://www.kaggle.com/c/porto-seguro-safe-driver-prediction pour avoir plus d'information.\n",
    "\n",
    "Vous l'aurez donc compris, il faut entraîner un prédicteur probabilistique pour avoir des probabilités durant la prédiction (ex: Regression Logistique, Bayes Naif, etc. et non SVM).\n",
    "\n",
    "Votre première tâche sera donc d'entraîner un classifieur sur l'ensemble d'entraînement et de prédire des probabilités sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCeLW6gdM56p"
   },
   "source": [
    "### Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiw7OqtWMh2P"
   },
   "source": [
    "Pour ce laboratoiree le but est surtout d'étudier la classification. La préparation des données est donc faite pour vous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13708,
     "status": "ok",
     "timestamp": 1526678119489,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "s7OoFcqBM560",
    "outputId": "cf6ba7ff-ce4f-497c-93b0-b2db0882bf8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates dropped.\n",
      "\n",
      "Under-sampling...\n",
      "Rate to undersample records with target=0: 0.34043569687437886\n",
      "Number of records with target=0 after undersampling: 195246\n",
      "\n",
      "Checking missing values...\n",
      "Variable ps_ind_02_cat has 103 records (0.05%) with missing values\n",
      "Variable ps_ind_04_cat has 51 records (0.02%) with missing values\n",
      "Variable ps_ind_05_cat has 2256 records (1.04%) with missing values\n",
      "Variable ps_reg_03 has 38580 records (17.78%) with missing values\n",
      "Variable ps_car_01_cat has 62 records (0.03%) with missing values\n",
      "Variable ps_car_02_cat has 2 records (0.00%) with missing values\n",
      "Variable ps_car_03_cat has 148367 records (68.39%) with missing values\n",
      "Variable ps_car_05_cat has 96026 records (44.26%) with missing values\n",
      "Variable ps_car_07_cat has 4431 records (2.04%) with missing values\n",
      "Variable ps_car_09_cat has 230 records (0.11%) with missing values\n",
      "Variable ps_car_11 has 1 records (0.00%) with missing values\n",
      "Variable ps_car_14 has 15726 records (7.25%) with missing values\n",
      "In total, there are 12 variables with missing values\n",
      "\n",
      "Dropping variables with more than 40% of missing values...\n",
      "Dropping ps_car_03_cat, ps_car_05_cat\n",
      "\n",
      "Replacing missing values...\n",
      "Replacing missing values of ps_reg_03 by mean\n",
      "Replacing missing values of ps_car_11 by mode\n",
      "Replacing missing values of ps_car_14 by mean\n",
      "\n",
      "Checking the cardinality of the categorical variables...\n",
      "Variable ps_ind_02_cat has 5 distinct values\n",
      "Variable ps_ind_04_cat has 3 distinct values\n",
      "Variable ps_ind_05_cat has 8 distinct values\n",
      "Variable ps_car_01_cat has 13 distinct values\n",
      "Variable ps_car_02_cat has 3 distinct values\n",
      "Variable ps_car_04_cat has 10 distinct values\n",
      "Variable ps_car_06_cat has 18 distinct values\n",
      "Variable ps_car_07_cat has 3 distinct values\n",
      "Variable ps_car_08_cat has 2 distinct values\n",
      "Variable ps_car_09_cat has 6 distinct values\n",
      "Variable ps_car_10_cat has 3 distinct values\n",
      "Variable ps_car_11_cat has 104 distinct values\n",
      "\n",
      "Dummification...\n",
      "Before dummification we have 55 variables in train\n",
      "After dummification we have 107 variables in train\n",
      "Before dummification we have 55 variables in test\n",
      "After dummification we have 107 variables in test\n",
      "\n",
      "Creating interaction variables...\n",
      "Before creating interactions we have 107 variables in train\n",
      "After creating interactions we have 162 variables in train\n",
      "Before creating interactions we have 107 variables in test\n",
      "After creating interactions we have 162 variables in test\n"
     ]
    }
   ],
   "source": [
    "data_path='./input'\n",
    "\n",
    "train = pd.read_csv(data_path+'/train.csv')\n",
    "test = pd.read_csv(data_path+'/test.csv')\n",
    "\n",
    "prep = prepare_data(train,test)\n",
    "train, targets, test = prep(True,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oz-seIskM57Y"
   },
   "source": [
    "## Classifieur classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pga1BAImNRR7"
   },
   "source": [
    "**QUESTION 1:** Entraînez un classifieur classique sur l'ensemble X_train et retournez le score Gini pour cet ensemble, puis prédisez les probabilités sur l'ensemble de test. Pour avoir les meilleurs résultats possibles (score gini), faites une recherche d'hyper-paramètres (ex:GridSearchCV) et **montrez votre travail**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9aKgkHyShyXB"
   },
   "outputs": [],
   "source": [
    "X, y = train.as_matrix()[:,1:], targets.as_matrix()\n",
    "X, y = shuffle(X,y)\n",
    "cutoff = int(len(X)*0.9)\n",
    "\n",
    "X_train_valid, y_train_valid = X[:cutoff], y[:cutoff]\n",
    "X_test, y_test = X[cutoff:], y[cutoff:]\n",
    "X_sub = test.as_matrix()[:,1:]\n",
    "\n",
    "del X, y, train, targets, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons essayé plusieurs types de classifieurs sur cette tâche. La table suivante montre les scores Gini obtenus pour ceux-ci avec le temps pris pour chaque algorithme avec leurs valeurs par défaut :\n",
    "\n",
    "\n",
    "|Classifieur                    | Score Gini | Temps (s) |\n",
    "|-------------------------------|------------|-----------|\n",
    "|Régression Logistique          |      0.270 |21.91      |\n",
    "|Classifieur SGD                |      0.265 |   27.62    |\n",
    "|Bernoulli Naive Bayes          |      0.224 |      0.45 |\n",
    "|Gaussian Naive Bayes           |      0.224 |      0.48 |\n",
    "|Multinomial Naive Bayes        |      0.227 |      0.10 |\n",
    "|Arbre de décision              |      0.047 |      23.28|\n",
    "|Réseau de neurones MLP         |      0.218 |  240.85   |\n",
    "|Linear Discriminant Analysis   |      0.271 |  2.20     |\n",
    "|Quadratic Discriminant Analysis|      0.156 |  1.60     |\n",
    "|K Neighbors                    |      0.032 | 649.59    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96528,
     "status": "ok",
     "timestamp": 1526674217376,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "m2MVSySqpWkU",
    "outputId": "e057e9e8-b63f-4d83-e8ae-38a3b545247f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for test set : 0.265\n",
      "Time elapsed : 27.62 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Initialiser votre classifieur\n",
    "# clf = LogisticRegression()    \n",
    "# clf = KNeighborsClassifier()  \n",
    "clf = SGDClassifier(loss='log', max_iter=250)  \n",
    "# clf = BernoulliNB()              \n",
    "# clf = GaussianNB()     \n",
    "# clf = MultinomialNB()  \n",
    "# clf = DecisionTreeClassifier()  \n",
    "# clf = MLPClassifier()          \n",
    "# clf = LinearDiscriminantAnalysis() \n",
    "# clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Entraîner le classifieur\n",
    "clf.fit(X_train_valid,y_train_valid)\n",
    "\n",
    "# Calculer les probabilitées prédites sur l'ensemble de test\n",
    "probs = clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "# Calculer le score Gini\n",
    "score_test = eval_gini(y_test,probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n",
    "\n",
    "# Temps pris pour le classifieur\n",
    "print(\"Time elapsed : %.2f s\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons aussi fait une recherche d'hyperparamètres à l'aide de GridSearchCV pour la régression logistique, les arbres de décision et les réseaux de neurones de type MLP. Les résultats sont présentés dans la table ci-dessous:\n",
    "\n",
    "| Classifieur                   | Meilleure configuration                 | Score Gini obtenu |\n",
    "|-------------------------------|-----------------------------------------|----------------------------|\n",
    "| Logistic Regression           |  max_iter=100, alpha=1e-3              |       0.278                |\n",
    "| Decision Tree                 |  max_depth=3                            |       0.180                |\n",
    "| Réseau de neurones MLP        |  hidden_layers=(100, 100), batch_size=32|       0.278                |\n",
    "\n",
    "On note que SGDClassifier utilisant la perte `log` est une régression logistique utilisant la descente de gradient pour apprendre ses paramètres. De cette façon, il est possible d'ajuster plus d'hyperparamètres. Cependant, le meilleur classifieur est obtenu en fonction de l'accuracy et non du score Gini, ce qui explique pourquoi le score Gini obtenu est plus bas que celui obtenu précédemment.\n",
    "\n",
    "On voit que les meilleurs résultats obtenus sont avec un réseau de neurones avec deux couches cachées de 100 neurones. Cependant, ce type d'algorithme est beaucoup plus long à entraîner que les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 27.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'alpha': 0.001, 'max_iter': 100}\n",
      "Best score for test set : 0.278\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_iter\": [50, 100, 250, 500, 1000],\n",
    "          \"alpha\": [0.001, 0.0001, 0.00001, 0.000001]}\n",
    "\n",
    "clf = GridSearchCV(SGDClassifier(loss='log', n_jobs=-1), params, verbose=1)\n",
    "\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "probs = clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "score_test = eval_gini(y_test, probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   29.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'max_depth': 3}\n",
      "Best score for test set : 0.180\n",
      "Time elapsed : 33.06 s\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_depth\": [3, 5, 7, None]}\n",
    "\n",
    "start = time.time()\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), params, n_jobs=-1, verbose=1)\n",
    "\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "probs = clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "score_test = eval_gini(y_test, probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n",
    "\n",
    "# Temps pris pour le classifieur\n",
    "print(\"Time elapsed : %.2f s\" % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 305.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'batch_size': 32, 'hidden_layer_sizes': (100, 100)}\n",
      "Best score for test set : 0.278\n",
      "Time elapsed : 18859.44 s\n"
     ]
    }
   ],
   "source": [
    "params = {\"hidden_layer_sizes\": [(100, ), (200, ), (100, 100)],\n",
    "          \"batch_size\": [32, 64, 128]}\n",
    "\n",
    "start = time.time()\n",
    "clf = GridSearchCV(MLPClassifier(), params, n_jobs=-1, verbose=1)\n",
    "\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "probs = clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "score_test = eval_gini(y_test, probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n",
    "\n",
    "# Temps pris pour le classifieur\n",
    "print(\"Time elapsed : %.2f s\" % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TX7rRaDBQcKI"
   },
   "source": [
    "## Méthodes d'ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmC1gteVQsBl"
   },
   "source": [
    "Les méthodes d'ensembles sont très populaires dans le machine learning pour donner de très bons résultats rapidement avec des classifieurs simples. Dans la communauté Kaggle, particulièrement, ces méthodes sont souvent utilisées et ne nécessitent pas des capacités de computation très élevées contraîrement aux réseaux de neurones.\n",
    "\n",
    "Le principe général est simple: combiner plusieurs classifieurs simples afin d'obtenir un classifieur qui généralise mieux. \n",
    "\n",
    "Pour rappel, l'erreur de généralisation peut se décomposer en deux termes: \n",
    "\n",
    "\n",
    "*   Le biais: si la famille de fonction considérée ne contient pas la fonction idéale\n",
    "*   La variance: variabilité dans la focntion trouvée due à la variabilité de l'ensemble de données\n",
    "\n",
    "La deuxième partie du laboratoire consite donc à se familiariser avec ces méthodes et de déterminer si elles aident pour cette tâche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IR06_EyTExZz"
   },
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gk3E58RBP4oS"
   },
   "source": [
    "Le boosting a principalement pour but de réduire le biais. Sont principe est le suivant:\n",
    "\n",
    "\n",
    "1.   Entraîner un classifieur simple $h_1$ sur l'ensemble de donnée initial et prédire les classes.\n",
    "2.   Déterminer les échantillons mal classés et entraîner un autre classifieur $h_{t+1}$ en augmentant l'importance de ces échantillons dans le calcul de la perte.\n",
    "3.   Combiner tous les classifieurs avec des poids différents ($\\alpha_t$) pour chaque classifieurs pour obtenir les prédictions.\n",
    "\n",
    "  Pour une prédiction binaire {-1,1}:      $H(x) = sign(\\sum_{t=1}^T \\alpha_t h_t(x))$\n",
    "\n",
    "4.   Répéter étapes 2, 3 N fois.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eed7iERyZOW4"
   },
   "source": [
    "**QUESTION 2**: Reprenez le classifieur précédent en rajoutant du boosting et comparez les résultats. \n",
    "\n",
    "HINT: Extreme Gradient Boosting semble fonctionner particulièrement bien pour ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons essayé plusieurs types de méthodes d'ensemble boostés avec différent classifieurs comme algorithme de base. Les résultats obtenus sont présentés au tableau suivant :\n",
    "\n",
    "| Méthode de boosting             | Algorithme de base         |Configuration optimale       | Gini sur Valid set     | Gini sur Test set      |\n",
    "|---------------------------------|----------------------------|-----------------------------|------------------------|------------------------|\n",
    "|  eXtreme Gradient Boosting      |    Arbre de decision       | max_depth=5, lr=0.025, n_estimators=200         |  0.271                 |  0.285                 |\n",
    "|  eXtreme Gradient Boosting      |    Regression logistique   |              lr=0.5, n_estimators=200           |  0.180                 |  0.265                 |\n",
    "|   AdaBoost                      | Arbre de decision          |      lr=0.5, n_estimators=50                    |      0.267             |   0.254                |\n",
    "|   AdaBoost                      | Regression logistique      |      lr=1.0, n_estimators=100                   |      0.258             |   0.248                |\n",
    "|   Gradient Boosting             | Arbre de decision          |  max_depth=3, lr=0.025, n_estimators=100        |      0.257             |   0.250                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "epbbNLbxG5xt"
   },
   "outputs": [],
   "source": [
    "def BoostedClassifier(X_train_, y_train_, X_valid_, y_valid_, clf):\n",
    "    # Initialiser le classifieur Boosté\n",
    "\n",
    "#     clf = XGBClassifier()                    # valid : 0.281, test : 0.285\n",
    "#     clf = XGBClassifier(booster='gblinear')\n",
    "#     clf = AdaBoostClassifier()               # valid : 0.259, test : 0.261\n",
    "#     clf = GradientBoostingClassifier()       # valid : 0.277, test = 0.285\n",
    "#     clf = XGBClassifier(booster='gblinear')    # valid : 0.102, test = 0.115   \n",
    "\n",
    "    # Entraîner le classifieur avec la métrique gini_xgb (si-possible)\n",
    "    clf.fit(X_train_, y_train_)\n",
    "\n",
    "    # Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de validation\n",
    "    valid_probs = clf.predict_proba(X_valid_)[:,-1]\n",
    "\n",
    "    # Calculer le score Gini pour les probabilités valid_probs\n",
    "    score_valid = eval_gini(y_valid_,valid_probs)\n",
    "    print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "    \n",
    "    return score_valid\n",
    "\n",
    "    # we should only evaluate the test score after the hyperparameter search!\n",
    "#     # Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de test\n",
    "#     test_probs = clf.predict_proba(X_test)[:,-1]\n",
    "    \n",
    "#     score_test = eval_gini(y_test,test_probs)\n",
    "#     print(\"Best score for test set : %0.3f\" % score_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer X_train_valid, y_train_valid en X_train, X_valid et y_train, y_valid avec un split de 0.9 (90% des données dans train et 10% dans test)\n",
    "cutoff = int(len(X_train_valid)*0.9)\n",
    "X_train, y_train = X_train_valid[:cutoff], y_train_valid[:cutoff]\n",
    "X_valid, y_valid = X_train_valid[cutoff:], y_train_valid[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431438,
     "status": "ok",
     "timestamp": 1526678553983,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "x3T4mJx3ns2u",
    "outputId": "abc980c0-9baf-439c-d968-e7d6e04a6e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth : 3 ----- Learning rate : 1.000 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.196\n",
      "Max depth : 3 ----- Learning rate : 0.500 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.245\n",
      "Max depth : 3 ----- Learning rate : 0.250 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.256\n",
      "Max depth : 3 ----- Learning rate : 0.100 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.268\n",
      "Max depth : 3 ----- Learning rate : 0.050 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.256\n",
      "Max depth : 3 ----- Learning rate : 0.025 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.242\n",
      "Max depth : 3 ----- Learning rate : 0.010 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.229\n",
      "Max depth : 5 ----- Learning rate : 1.000 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.130\n",
      "Max depth : 5 ----- Learning rate : 0.500 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.192\n",
      "Max depth : 5 ----- Learning rate : 0.250 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.241\n",
      "Max depth : 5 ----- Learning rate : 0.100 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.264\n",
      "Max depth : 5 ----- Learning rate : 0.050 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.270\n",
      "Max depth : 5 ----- Learning rate : 0.025 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.260\n",
      "Max depth : 5 ----- Learning rate : 0.010 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.244\n",
      "Max depth : 3 ----- Learning rate : 1.000 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.179\n",
      "Max depth : 3 ----- Learning rate : 0.500 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.226\n",
      "Max depth : 3 ----- Learning rate : 0.250 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.244\n",
      "Max depth : 3 ----- Learning rate : 0.100 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.267\n",
      "Max depth : 3 ----- Learning rate : 0.050 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.269\n",
      "Max depth : 3 ----- Learning rate : 0.025 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.257\n",
      "Max depth : 3 ----- Learning rate : 0.010 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.238\n",
      "Max depth : 5 ----- Learning rate : 1.000 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.104\n",
      "Max depth : 5 ----- Learning rate : 0.500 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.157\n",
      "Max depth : 5 ----- Learning rate : 0.250 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.215\n",
      "Max depth : 5 ----- Learning rate : 0.100 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.261\n",
      "Max depth : 5 ----- Learning rate : 0.050 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.268\n",
      "Max depth : 5 ----- Learning rate : 0.025 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.271\n",
      "Max depth : 5 ----- Learning rate : 0.010 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.254\n",
      "Best configuration : max_depth = 5, learning_rate = 0.03,  Nb of estimators = 200\n",
      "Best score for test set : 0.285\n"
     ]
    }
   ],
   "source": [
    "# Appeler le BoostedClassifier\n",
    "\n",
    "# gridsearchcv doesn't work well with xgbclassifier\n",
    "best_valid_score=0\n",
    "best_max_depth=0\n",
    "best_learning_rate=0\n",
    "best_n_estimators=0\n",
    "for n_estimators in [100, 200]:\n",
    "    for max_depth in [3, 5]:\n",
    "        for learning_rate in [1, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01]:\n",
    "            print(\"Max depth : %d ----- Learning rate : %.3f ----- Nb of estimators : %d\"%(max_depth, learning_rate, n_estimators))\n",
    "            estimator = XGBClassifier(booster='gbtree', max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "\n",
    "            valid_score = BoostedClassifier(X_train, y_train, X_valid, y_valid, clf=estimator)\n",
    "\n",
    "            if valid_score > best_valid_score:\n",
    "                best_valid_score=valid_score\n",
    "                best_max_depth = max_depth\n",
    "                best_learning_rate = learning_rate\n",
    "                best_n_estimators = n_estimators\n",
    "            \n",
    "# Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de test\n",
    "\n",
    "estimator = XGBClassifier(booster='gbtree', max_depth=best_max_depth, learning_rate=best_learning_rate,\n",
    "                          n_estimators=best_n_estimators)\n",
    "estimator.fit(X_train, y_train)\n",
    "test_probs = estimator.predict_proba(X_test)[:,-1]\n",
    "\n",
    "print(\"Best configuration : max_depth = %d, learning_rate = %.2f,  Nb of estimators = %d\" % (best_max_depth, best_learning_rate, best_n_estimators))\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate : 2.500 ----- Nb of estimators : 100\n",
      "Best score for valid set : -0.014\n",
      "Learning rate : 1.000 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.180\n",
      "Learning rate : 0.500 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.179\n",
      "Learning rate : 0.250 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.148\n",
      "Learning rate : 0.100 ----- Nb of estimators : 100\n",
      "Best score for valid set : 0.097\n",
      "Learning rate : 2.500 ----- Nb of estimators : 200\n",
      "Best score for valid set : -0.014\n",
      "Learning rate : 1.000 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.180\n",
      "Learning rate : 0.500 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.180\n",
      "Learning rate : 0.250 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.174\n",
      "Learning rate : 0.100 ----- Nb of estimators : 200\n",
      "Best score for valid set : 0.126\n",
      "Best configuration : learning_rate = 0.500,  Nb of estimators = 200\n",
      "Best score for test set : 0.265\n"
     ]
    }
   ],
   "source": [
    "# Appeler le BoostedClassifier\n",
    "\n",
    "# gridsearchcv doesn't work well with xgbclassifier\n",
    "best_valid_score=0\n",
    "best_n_estimators=0\n",
    "best_learning_rate=0\n",
    "for n_estimators in [100, 200]:\n",
    "    for learning_rate in [2.5, 1, 0.5, 0.25, 0.1]:\n",
    "        print(\"Learning rate : %.3f ----- Nb of estimators : %d\"%(learning_rate, n_estimators))\n",
    "        estimator = XGBClassifier(booster='gblinear', learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "\n",
    "        valid_score = BoostedClassifier(X_train, y_train, X_valid, y_valid, clf=estimator)\n",
    "\n",
    "        if valid_score > best_valid_score:\n",
    "            best_valid_score = valid_score\n",
    "            best_n_estimators = n_estimators\n",
    "            best_learning_rate = learning_rate\n",
    "\n",
    "# Calculer les probabilités prédites par le meilleur estimateur sur l'ensemble de test\n",
    "estimator = XGBClassifier(booster='gblinear', learning_rate=best_learning_rate,\n",
    "                          n_estimators=best_n_estimators)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "test_probs = clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "print(\"Best configuration : learning_rate = %.3f,  Nb of estimators = %d\" % ( best_learning_rate, best_n_estimators))\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed: 25.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'learning_rate': 0.5, 'n_estimators': 50}\n",
      "Best score for valid set : 0.267\n",
      "Best score for test set : 0.254\n"
     ]
    }
   ],
   "source": [
    "params = {\"n_estimators\": [50, 100, 200], \"learning_rate\": [1., 0.5, 0.25, 0.1, 0.05, 0.025, 0.01]}\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(), params, n_jobs=-1, verbose=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "valid_probs = clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "\n",
    "test_probs = clf.predict_proba(X_test)[:,-1]\n",
    "score_test = eval_gini(y_test, test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed: 46.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'learning_rate': 1.0, 'n_estimators': 100}\n",
      "Best score for valid set : 0.258\n",
      "Best score for test set : 0.248\n"
     ]
    }
   ],
   "source": [
    "params = {\"n_estimators\": [50, 100, 200], \"learning_rate\": [1., 0.5, 0.25, 0.1, 0.05, 0.025, 0.01]}\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(base_estimator=LogisticRegression()), params, n_jobs=-1, verbose=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "valid_probs = clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "\n",
    "test_probs = clf.predict_proba(X_test)[:,-1]\n",
    "score_test = eval_gini(y_test, test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed: 56.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : \n",
      "{'learning_rate': 0.025, 'max_depth': 3}\n",
      "Best score for valid set : 0.257\n",
      "Best score for test set : 0.250\n"
     ]
    }
   ],
   "source": [
    "params = { \"learning_rate\": [1., 0.5, 0.25, 0.1, 0.05, 0.025, 0.01], \"max_depth\":[3, 5]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), params, n_jobs=-1, verbose=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters : \")\n",
    "print(clf.best_params_)\n",
    "\n",
    "valid_probs = clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "\n",
    "test_probs = clf.predict_proba(X_test)[:,-1]\n",
    "score_test = eval_gini(y_test, test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fym6WDukEsRY"
   },
   "source": [
    "### Boosting + Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "is52ASGFZ5iK"
   },
   "source": [
    "La bagging est une méthode relativement simple qui a pour but de réduire la variance. Le principe est le suivant:\n",
    "1.   Séparer l'ensemble d'entraînement en N ensembles.\n",
    "2.   Entraîner N classifieurs simples sur les N ensembles.\n",
    "3.   Produire N sets de prédictions sur l'ensemble de validation avec ces prédicteurs \n",
    "4.   Faire la moyenne de ces prédictions\n",
    "\n",
    "      $H(x) = \\dfrac{1}{N} \\sum_{t=1}^N h_t(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmejXT_QbFnq"
   },
   "source": [
    "**QUESTION 3**: Utiliser la méthode du bagging en réutilisant l'algorithme boosté précédant. BONUS si vous obtenez plus de 0.3 de score Gini sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne garde que les deux meilleurs résultats obtenus précédemment, soit XGBClassifier utilisant les arbres de décision ainsi que XGBClassifier avec la régression logistique. Les hyperparamètres choisis sont ceux obtenus avec le grid search au numéro précédent. On fait le tout avec 20 estimateurs. On obtient les résultats suivants :\n",
    "\n",
    "| Estimateur de base                      | Valid Gini     | Test Gini     |\n",
    "|-----------------------------------------|----------------|---------------|\n",
    "| XGBClassifier avec arbres               |     0.261      |    0.259      |\n",
    "| XGBClassifier avec régression logistique|     0.163      |    0.182      |  \n",
    "\n",
    "Les résultats sont quelque peu décevants. On ne garde donc que XGBClassifier avec les arbres de décision en choisissant de nouveaux hyperparamètres pour avoir de meilleurs résultats. On utilise cette fois-ci seulement 12 estimateurs pour accélérer l'entraînement. On obtient les résultats suivants :\n",
    "\n",
    "| Estimateur de base                      |Hyperparamètres        | Valid Gini     | Test Gini     |\n",
    "|-----------------------------------------|-----------------------|----------------|---------------|\n",
    "|XGBClassifier avec arbres                | lr=0.05, max_depth=3   |     0.267      |    0.263      | \n",
    "| XGBClassifier avec arbres               | lr=0.1, max_depth=3   |     0.277      |    0.274      |\n",
    "|XGBClassifier avec arbres                | lr=0.5, max_depth=3   |     0.270      |    0.273      | \n",
    "|XGBClassifier avec arbres                | lr=1.0, max_depth=3   |     0.250      |    0.264      | \n",
    "|XGBClassifier avec arbres                | lr=0.05, max_depth=5   |     0.278      |    0.275      | \n",
    "| XGBClassifier avec arbres               | lr=0.1, max_depth=5   |     0.283      |    0.279      |\n",
    "|XGBClassifier avec arbres                | lr=0.5, max_depth=5   |     0.236      |    0.247      | \n",
    "\n",
    "Nous ne sommes donc pas en mesure d'obtenir un score meilleur que 0.279 utilisant le bagging ainsi que le extreme gradient boosting avec des arbres de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 5 for this parallel run (total 20)...\n",
      "Building estimator 1 of 5 for this parallel run (total 20)...\n",
      "Building estimator 1 of 5 for this parallel run (total 20)...\n",
      "Building estimator 1 of 5 for this parallel run (total 20)...\n",
      "Building estimator 2 of 5 for this parallel run (total 20)...\n",
      "Building estimator 2 of 5 for this parallel run (total 20)...\n",
      "Building estimator 2 of 5 for this parallel run (total 20)...\n",
      "Building estimator 2 of 5 for this parallel run (total 20)...\n",
      "Building estimator 3 of 5 for this parallel run (total 20)...\n",
      "Building estimator 3 of 5 for this parallel run (total 20)...\n",
      "Building estimator 3 of 5 for this parallel run (total 20)...\n",
      "Building estimator 3 of 5 for this parallel run (total 20)...\n",
      "Building estimator 4 of 5 for this parallel run (total 20)...\n",
      "Building estimator 4 of 5 for this parallel run (total 20)...\n",
      "Building estimator 4 of 5 for this parallel run (total 20)...\n",
      "Building estimator 4 of 5 for this parallel run (total 20)...\n",
      "Building estimator 5 of 5 for this parallel run (total 20)...\n",
      "Building estimator 5 of 5 for this parallel run (total 20)...\n",
      "Building estimator 5 of 5 for this parallel run (total 20)...\n",
      "Building estimator 5 of 5 for this parallel run (total 20)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed: 10.3min remaining: 10.3min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed: 10.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed: 10.4min finished\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for valid set : 0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for test set : 0.259\n"
     ]
    }
   ],
   "source": [
    "# Faire le Bagging de votre clasifieur boosté.\n",
    "\n",
    "base_estimator = XGBClassifier(booster='gbtree',\n",
    "                               learning_rate=0.025,\n",
    "                               max_depth=5)\n",
    "\n",
    "bagged_clf = BaggingClassifier(base_estimator=base_estimator,\n",
    "                               n_estimators=20, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=10)\n",
    "\n",
    "bagged_clf.fit(X_train, y_train)\n",
    "\n",
    "valid_probs = bagged_clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "  \n",
    "# Faire la moyenne des probabilitées sur l'ensemble de test\n",
    "test_probs = bagged_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini sur l'ensemble de test\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 16803
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2059099,
     "status": "ok",
     "timestamp": 1526681790316,
     "user": {
      "displayName": "Alexandre Dos Santos",
      "photoUrl": "//lh3.googleusercontent.com/-x49DIhX_uz0/AAAAAAAAAAI/AAAAAAAAAKg/GJ1QqAk9EMo/s50-c-k-no/photo.jpg",
      "userId": "105252364393852282664"
     },
     "user_tz": 240
    },
    "id": "o3ovg83wclaF",
    "outputId": "dbc282b8-5fc0-420f-b3e6-139fd2daad03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 3 for this parallel run (total 10)...\n",
      "Building estimator 1 of 3 for this parallel run (total 10)...\n",
      "Building estimator 1 of 2 for this parallel run (total 10)...\n",
      "Building estimator 1 of 2 for this parallel run (total 10)...\n",
      "Building estimator 2 of 3 for this parallel run (total 10)...\n",
      "Building estimator 2 of 3 for this parallel run (total 10)...\n",
      "Building estimator 2 of 2 for this parallel run (total 10)...\n",
      "Building estimator 2 of 2 for this parallel run (total 10)...\n",
      "Building estimator 3 of 3 for this parallel run (total 10)...\n",
      "Building estimator 3 of 3 for this parallel run (total 10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  1.1min remaining:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for valid set : 0.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for test set : 0.182\n"
     ]
    }
   ],
   "source": [
    "# Faire le Bagging de votre clasifieur boosté.\n",
    "\n",
    "base_estimator = XGBClassifier(booster='gblinear',\n",
    "                               learning_rate=0.5,\n",
    "                               n_estimators=200)\n",
    "\n",
    "bagged_clf = BaggingClassifier(base_estimator=base_estimator,\n",
    "                               n_estimators=10, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=10)\n",
    "\n",
    "bagged_clf.fit(X_train, y_train)\n",
    "\n",
    "valid_probs = bagged_clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "  \n",
    "# Faire la moyenne des probabilitées sur l'ensemble de test\n",
    "test_probs = bagged_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini sur l'ensemble de test\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 3 for this parallel run (total 12)...\n",
      "Building estimator 1 of 3 for this parallel run (total 12)...\n",
      "Building estimator 1 of 3 for this parallel run (total 12)...\n",
      "Building estimator 1 of 3 for this parallel run (total 12)...\n",
      "Building estimator 2 of 3 for this parallel run (total 12)...\n",
      "Building estimator 2 of 3 for this parallel run (total 12)...\n",
      "Building estimator 2 of 3 for this parallel run (total 12)...\n",
      "Building estimator 2 of 3 for this parallel run (total 12)...\n",
      "Building estimator 3 of 3 for this parallel run (total 12)...\n",
      "Building estimator 3 of 3 for this parallel run (total 12)...\n",
      "Building estimator 3 of 3 for this parallel run (total 12)...\n",
      "Building estimator 3 of 3 for this parallel run (total 12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  6.1min remaining:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.1min finished\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for valid set : 0.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for test set : 0.247\n"
     ]
    }
   ],
   "source": [
    "# Faire le Bagging de votre clasifieur boosté.\n",
    "\n",
    "# trying out new hyperparameters\n",
    "\n",
    "base_estimator = XGBClassifier(booster='gbtree',\n",
    "                               learning_rate=0.5,\n",
    "                               max_depth=5)\n",
    "\n",
    "bagged_clf = BaggingClassifier(base_estimator=base_estimator,\n",
    "                               n_estimators=12, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=10)\n",
    "\n",
    "bagged_clf.fit(X_train, y_train)\n",
    "\n",
    "valid_probs = bagged_clf.predict_proba(X_valid)[:,-1]\n",
    "score_valid = eval_gini(y_valid, valid_probs)\n",
    "print(\"Best score for valid set : %0.3f\" % score_valid)\n",
    "  \n",
    "# Faire la moyenne des probabilitées sur l'ensemble de test\n",
    "test_probs = bagged_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculer le score Gini sur l'ensemble de test\n",
    "score_test = eval_gini(y_test,test_probs)\n",
    "print(\"Best score for test set : %0.3f\" % score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVN6fq58I4V4"
   },
   "source": [
    "# Pour soumettre les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SoiQHMsx68Gv"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"submission.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "    csv_writer.writerow([\"id\",\"target\"])\n",
    "    for i, prob in enumerate(test_probs):\n",
    "        csv_writer.writerow([i,prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIYDhWeiPCQ1"
   },
   "source": [
    "Excecutez la commande:\n",
    "\n",
    "`kaggle competitions submit -c porto-seguro-safe-driver-prediction -f submission.csv -m \"Message\"`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Laboratoire_2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
